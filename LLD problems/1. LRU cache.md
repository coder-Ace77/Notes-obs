
---

LRU cache can be implemented using the Linked list and hashmap. 

### Simplest java solution

```java
package cache;

public interface Cache {
    public void put(String key,String value);
    public String get(String key);
}
```

```java
package cache;
import java.util.HashMap;
import datastructures.*;

public class LRUCache implements Cache{

    private LinkedList linkedList;
    private HashMap<String,ListNode> hashMap;
    private int size=0;
    private int capacity=0;

    public LRUCache(int capacity){
        linkedList=new LinkedList();
        size = 0;
        this.capacity=capacity;
        hashMap = new HashMap<>();
    }

    @Override
    public void put(String key, String value) {
        if (hashMap.containsKey(key)) {
            ListNode node = hashMap.get(key);
            node.val = value; 
            linkedList.moveToHead(node); 
        } else {
            if (size >= capacity){
                ListNode lru = linkedList.back();
                if (lru != null) {
                    hashMap.remove(lru.key);
                    linkedList.delete(lru);
                    size--;
                }
            }
            ListNode newNode = new ListNode(key, value);
            linkedList.addFirst(newNode);
            hashMap.put(key, newNode);
            size++;
        }
    }

    @Override
    public String get(String key) {
        if (!hashMap.containsKey(key)) return "";
        
        ListNode node = hashMap.get(key);
        linkedList.moveToHead(node); 
        return node.val;
    }    
}
```

It uses a linked list which is 

```java
package datastructures;

public class LinkedList {
    private final ListNode head;
    private final ListNode tail;

    public LinkedList() {
        head = new ListNode("head", "sentinel");
        tail = new ListNode("tail", "sentinel");
        head.next = tail;
        tail.prev = head;
    }

    public ListNode front() {
        return head.next == tail ? null : head.next;
    }

    public ListNode back() {
        return tail.prev == head ? null : tail.prev;
    }

    // New: Inserts an existing node to the front
    public void addFirst(ListNode node) {
        node.next = head.next;
        node.prev = head;
        head.next.prev = node;
        head.next = node;
    }

    // Simplified delete: No more null checks needed for head/tail!
    public void delete(ListNode node) {
        if (node == null || node == head || node == tail) return;
        node.prev.next = node.next;
        node.next.prev = node.prev;
    }

    public void moveToHead(ListNode node) {
        delete(node);
        addFirst(node);
    }
}
```

Now with this we have basic LRU cache done. However this is only single threaded and will not work in mulithreaded environments. 

## LRU cache in multithreaded environments

### Locking solution

First and simplest solution is to put some kind of lock like `ReentrantLock` or `semaphore`. 

```java
package cache;
import java.util.HashMap;
import java.util.concurrent.locks.ReentrantLock;

import datastructures.*;

public class LRULockbuffer implements Cache{

    private LinkedList linkedList;
    private HashMap<String,ListNode> hashMap;
    private int size=0;
    private int capacity=0;
    private final ReentrantLock lock = new ReentrantLock();

    public LRULockbuffer(int capacity){
        linkedList=new LinkedList();
        size = 0;
        this.capacity=capacity;
        hashMap = new HashMap<>();
    }

    @Override
    public void put(String key, String value) {
        lock.lock();
        try{
            if (hashMap.containsKey(key)) {
            ListNode node = hashMap.get(key);
            node.val = value; 
            linkedList.moveToHead(node); 
            } else {
                if (size >= capacity){
                    ListNode lru = linkedList.back();
                    if (lru != null) {
                        hashMap.remove(lru.key);
                        linkedList.delete(lru);
                        size--;
                    }
                }
                ListNode newNode = new ListNode(key, value);
                linkedList.addFirst(newNode);
                hashMap.put(key, newNode);
                size++;
            }
        }finally{
            lock.unlock();
        }
        
    }

    @Override
    public String get(String key) {
        lock.lock();
        try{
            if (!hashMap.containsKey(key)) return "";
            ListNode node = hashMap.get(key);
            linkedList.moveToHead(node); 
            return node.val;
        }finally{
            lock.unlock();
        }
    
    }    
}
```

Now that we can do locking our cache will work in multithreaded environments but will be very slow due to locking being done. Essentially we have retricted our entire cache to be only one threaded. As everytime an operation is asked we lock it. Note that we can not use `ReadWrite` lock even if reads are much larger in frequency than writes reason being `read` here is again write due to `hashmap` and `linked` `list` changes. 

The big problem with LRU cache is that each read or write requires a key to come in the front. This operation usually can not be done atomically and so we are forced to lock the entire linked list for each read or write. There is no locking solution which can give better performance than simple reentrant lock in case of strict `LRU`. This is  the reason we are forced to use the relaxed `lru` meaning the key in front may not always be the most recent one. 

### Second solution concurrent hashmap+ ConcurrentLinkedDeque

Concurrent hashmap is the better solution than using locks the reason being hashmap gives us fine grained locking for us. Similarly we can use `ConcurrentLinkedDeque` which is lock free data strcuture. But the problem is we need to update two different collections atomically. So to stop this we need to which access or update `lock` both the hashmap and `linked deque`. This means again even if we use concurrent structures our solution will only be as good as reentrant lock one. 

### Third solution - Segmentation

The idea is to segregate the cache into multiple smaller caches. So that each of the smaller cache acts as independent cache we then use the hash fucntion to implement the smaller caches. This works well in case the keys are uniformly distributed. 

```java
import java.util.LinkedHashMap;
import java.util.Map;
import java.util.concurrent.locks.ReentrantLock;

public class SegmentedLRUCache<K, V> {
    private final Segment<K, V>[] segments;
    private final int mask;

    @SuppressWarnings("unchecked")
    public SegmentedLRUCache(int segmentCount, int capacityPerSegment) {
        // Ensure segmentCount is a power of two for efficient masking
        int n = 1;
        while (n < segmentCount) n <<= 1;
        
        this.segments = new Segment[n];
        this.mask = n - 1;
        
        for (int i = 0; i < n; i++) {
            segments[i] = new Segment<>(capacityPerSegment);
        }
    }

    private Segment<K, V> getSegment(Object key) {
        int hash = Math.abs(key.hashCode());
        return segments[hash & mask];
    }

    public V get(K key) {
        return getSegment(key).get(key);
    }

    public void put(K key, V value) {
        getSegment(key).put(key, value);
    }

    // Inner Segment Class
    private static class Segment<K, V> {
        private final int capacity;
        private final ReentrantLock lock = new ReentrantLock();
        private final LinkedHashMap<K, V> internalMap;

        public Segment(int capacity) {
            this.capacity = capacity;
            // 'true' enables access-order for LRU behavior
            this.internalMap = new LinkedHashMap<K, V>(capacity, 0.75f, true) {
                @Override
                protected boolean removeEldestEntry(Map.Entry<K, V> eldest) {
                    return size() > Segment.this.capacity;
                }
            };
        }

        public V get(K key) {
            lock.lock();
            try {
                return internalMap.get(key);
            } finally {
                lock.unlock();
            }
        }

        public void put(K key, V value) {
            lock.lock();
            try {
                internalMap.put(key, value);
            } finally {
                lock.unlock();
            }
        }
    }
}
```

If you have 16 segments, up to 16 threads can perform `get` or `put` operations concurrently without blocking each other, provided their keys hash to different segments. Note that here we have used linked hashmap which is basically `lru` build in.

### LRU cache using optimistic locking

Now can we do strict lru cache using optimistic locking. Actual answer is no the reason being `lru` cache requires us to add a pointer in the head on each and every instruction. Secondly we are also needed to lock both the linkedlist and `hashmap`. Therefore it will again fail. However we can still do a relaxed lru which is - 

**"Relaxed" LRU Cache** - When we read a value, we use an **optimistic read**. If the read succeeds, we return the value immediately. We then attempt to update the LRU order (move to front), but **only if the lock is free**. If the cache is busy, we skip the re-ordering. This sacrifices strict LRU exactness for massive performance gains.

It uses optimistic lock - `StampedLock`

This lock was introduced in java 8 and has two important features - 
- **`tryOptimisticRead()`**: Returns a stamp (a long version number). It does not block.
- **`validate(stamp)`**: Checks if the lock was acquired in write mode since the stamp was issued. Returns `true` if your data is still valid.

```java
import java.util.HashMap;
import java.util.Map;
import java.util.concurrent.locks.StampedLock;

public class OptimisticLRUCache<K, V> {

    private final int capacity;
    private final Map<K, Node<K, V>> map;
    private final StampedLock lock = new StampedLock();
    
    // Pointers for Doubly Linked List
    private Node<K, V> head;
    private Node<K, V> tail;

    public OptimisticLRUCache(int capacity) {
        this.capacity = capacity;
        this.map = new HashMap<>(capacity);
    }

    /**
     * Optimistic GET
     * 1. Try to read the value without blocking.
     * 2. If successful, return value.
     * 3. Attempt to update LRU order separately (Relaxed LRU).
     */
    public V get(K key) {
        // 1. Attempt Optimistic Read
        long stamp = lock.tryOptimisticRead();
        
        // Read from map (Note: Standard HashMap is not thread-safe, 
        // but if validate() passes, it means no structural changes happened during read)
        Node<K, V> node = map.get(key);

        // 2. Validate the stamp
        if (!lock.validate(stamp)) {
            // Optimistic read failed (someone was writing). 
            // Fallback to Pessimistic Read Lock.
            stamp = lock.readLock();
            try {
                node = map.get(key);
            } finally {
                lock.unlockRead(stamp);
            }
        }

        if (node == null) {
            return null;
        }

        // 3. Relaxed LRU Promotion
        // We have the value, but we need to move it to the head.
        // Instead of blocking just to move it, we 'try' to get a write lock.
        // If the lock is busy, we skip the promotion to keep retrieval fast.
        final Node<K, V> targetNode = node;
        long writeStamp = lock.tryWriteLock();
        if (writeStamp != 0) {
            try {
                moveToHead(targetNode);
            } finally {
                lock.unlockWrite(writeStamp);
            }
        }

        return node.value;
    }

    /**
     * Standard PUT (Always requires Write Lock)
     */
    public void put(K key, V value) {
        long stamp = lock.writeLock(); // Blocking Write Lock
        try {
            Node<K, V> node = map.get(key);

            if (node != null) {
                // Update existing
                node.value = value;
                moveToHead(node);
            } else {
                // Insert new
                node = new Node<>(key, value);
                map.put(key, node);
                addToHead(node);

                if (map.size() > capacity) {
                    removeTail();
                }
            }
        } finally {
            lock.unlockWrite(stamp);
        }
    }

    // --- Helper Methods (Must be called inside Write Lock) ---
    private void moveToHead(Node<K, V> node) {
        if (node == head) return;

        // Unlink
        if (node.prev != null) node.prev.next = node.next;
        if (node.next != null) node.next.prev = node.prev;
        if (node == tail) tail = node.prev;

        // Link at Head
        node.next = head;
        node.prev = null;
        if (head != null) head.prev = node;
        head = node;
        if (tail == null) tail = node;
    }

    private void addToHead(Node<K, V> node) {
        node.next = head;
        node.prev = null;
        if (head != null) head.prev = node;
        head = node;
        if (tail == null) tail = node;
    }

    private void removeTail() {
        if (tail == null) return;
        
        map.remove(tail.key);
        if (tail.prev != null) {
            tail.prev.next = null;
            tail = tail.prev;
        } else {
            head = null;
            tail = null;
        }
    }

    // Doubly Linked List Node
    private static class Node<K, V> {
        K key;
        V value;
        Node<K, V> prev;
        Node<K, V> next;

        Node(K key, V value) {
            this.key = key;
            this.value = value;
        }
    }
}
```

Note again that locking was done again on the entire cache not on one key. This makes the cache doning again one access at a time. 

## State of the art: Caffine

The state-of-the-art approach to caching, exemplified by the Caffeine library in Java and Ristretto in Go, fundamentally shifts from a simple Least Recently Used (LRU) policy to an admission-aware policy known as Window TinyLFU (W-TinyLFU). Standard LRU suffers significantly from scan resistance, meaning a single large query or a "scan" of one-time-use items can flush out high-value, frequently accessed data. W-TinyLFU solves this by separating the cache into two distinct regions: a small admission window and a large main region. New items are always admitted into the small window first. When this window fills up, the item evicted from it doesn't just disappear; it competes for space in the main region. This competition is decided by a historical frequency filter, ensuring that only items with a proven track record of access are allowed to displace existing residents of the main cache.

The core of this frequency tracking is a probabilistic data structure called a Count-Min Sketch. You can think of the Count-Min Sketch as a highly space-efficient approximate frequency counter, similar to a Bloom Filter but for counting rather than just existence. It uses a fixed-size array of counters and multiple hash functions. When an item is accessed, it is hashed by these functions to several distinct positions in the array, and the counters at those positions are incremented. To retrieve the frequency of an item, the system hashes the key to find the same positions and takes the minimum value among those counters. Taking the minimum is crucial because hash collisions might artificially inflate the count in some positions, but the true count cannot be higher than the minimum observed value. This allows the cache to track the frequency of millions of items using a tiny amount of memory, typically just 4 bits per counter.

To prevent these frequency counters from growing indefinitely and becoming stale, the system implements a "freshness" mechanism. If old high-frequency counts were never reset, an item that was popular yesterday but is useless today would never be evicted. The solution is a periodic reset operation often triggered when the total number of accesses reaches a certain sample size (e.g., 10 times the cache size). At this point, all counters in the sketch are divided by two. This simple bit-shifting operation acts as an exponential decay function, preserving the relative popularity of items while ensuring that recent history eventually outweighs ancient history. This allows the cache to adapt quickly to changing access patterns.

The actual eviction logic is a "duel" between the admission window's victim and the main cache's victim. When the admission window is full and evicts a candidate, the system checks the Count-Min Sketch to compare the frequency of this candidate against the frequency of the item currently at the tail of the main region (the eviction candidate). If the new candidate has a higher frequency estimate, it is admitted into the main region, and the existing main region victim is evicted. If the new candidate has a lower or equal frequency, it is rejected entirely, and the main region remains unchanged. This is a profound change from standard LRU, where new items are always admitted; in W-TinyLFU, most new items are rejected immediately if they haven't been seen enough times to justify removing a known popular item.

To further optimize the main region, it is often implemented as a Segmented LRU (SLRU). The SLRU divides the main cache into a "probationary" segment and a "protected" segment. Items promoted from the admission window start in the probationary segment. If they are accessed again while in probation, they are promoted to the protected segment. When the protected segment fills up, items overflow back into the probationary segment, giving them one last chance to be accessed before becoming eligible for eviction. This structure creates a firewall for the most valuable data, ensuring that transient data or "noise" rarely impacts the core working set of the application. The combination of the Count-Min Sketch for frequency history, the Window for recency, and the SLRU for retention creates a cache that hits consistently higher hit rates than pure LRU across almost all real-world workloads.

The final piece of the architecture is the use of a "Doorkeeper" Bloom filter placed before the Count-Min Sketch. Before incrementing the counters in the main sketch, the system checks the Doorkeeper. If the item is not in the Doorkeeper, it is inserted there but the main counters are not touched. Only if the item is already present in the Doorkeeper (meaning it has been seen at least once before) are the main frequency counters incremented. This filters out the vast majority of "one-hit wonders" (items seen only once) from polluting the frequency sketch, significantly improving the accuracy of the frequency estimates for the items that actually matter. This layered approach of Doorkeeper, Frequency Sketch, and Window SLRU defines the modern standard for high-performance caching.

